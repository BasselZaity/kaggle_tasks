{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- multi-task classification with 6 classes\n",
    "- training set size: 159,571\n",
    "- few examples (out of 159,571) associated with at least one of these classes\n",
    "\n",
    "- max number of terms in a single comment is 1403"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "tokenizer=TreebankWordTokenizer()\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "stemmer=EnglishStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train=pd.read_csv('./data/train.csv')\n",
    "test=pd.read_csv('./data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processed data 1.0\n",
    "\n",
    "- Tokenization (TreeBankWordTockenizer)\n",
    "- Stemmer for English Language (snowball)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def processText1_0(inpt):\n",
    "    inpt=inpt.lower()\n",
    "    tokens=tokenizer.tokenize(inpt)\n",
    "    stems=[stemmer.stem(token.decode('utf8')) for token in tokens]\n",
    "    return ' '.join(stems)\n",
    "    \n",
    "train.comment_text=train.comment_text.apply(processText1_0)\n",
    "test.comment_text=test.comment_text.apply(processText1_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.to_csv('data/train_processed_1.0.csv', encoding='utf-8')\n",
    "test.to_csv('data/test_processed_1.0.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processed data 2.0\n",
    "\n",
    "- Tokenization (TreeBankWordTockenizer)\n",
    "- lemmatizer (WordNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def processText2_0(inpt):\n",
    "    inpt=inpt.lower()\n",
    "    tokens=tokenizer.tokenize(inpt)\n",
    "    lemmas=[lemmatizer.lemmatize(token.decode('utf8')) for token in tokens]\n",
    "    return ' '.join(lemmas)\n",
    "    \n",
    "train.comment_text=train.comment_text.apply(processText2_0)\n",
    "test.comment_text=test.comment_text.apply(processText2_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.to_csv('data/train_processed_2.0.csv', encoding='utf-8')\n",
    "test.to_csv('data/test_processed_2.0.csv', encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
